{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from IPython.display import HTML, display , Markdown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"E:\\\\G- 1539 Blue Cat Seagate 500 GB\\\\Drive 2\\\\upgrad\\\\lending\\\\\"\n",
    "lending_file = base_path + \"loan.zip\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Read File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lend_df = pd.read_csv(lending_file)\n",
    "pd_lend_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Find and Drop empty columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lend_df.dropna(axis = 1, how = 'all' ,inplace = True)\n",
    "pd_lend_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Find and Drop Columns with unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lend_df=pd_lend_df.loc[:,pd_lend_df.nunique()>1]\n",
    "pd_lend_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # loan_status main target column  with object type, a quick look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lend_df['loan_status'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find unique values and keep relevant\n",
    "pd_lend_df['loan_status'] .unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loan_status of current means ongoing loan and is very small no. , dropping such rows and change type\n",
    "pd_lend_df = pd_lend_df[~(pd_lend_df['loan_status'] == \"Current\")]\n",
    "pd_lend_df['loan_status'] = pd_lend_df['loan_status'].astype('category')\n",
    "# out_prncp_inv and out_prncp are only related to current loans, dropping them\n",
    "pd_lend_df.drop(columns = ['out_prncp_inv','out_prncp'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Drop columns with < 30% non-null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From info, it is seen that there are 2 columns with , 30% non-null data , drop them\n",
    "pd_lend_df.drop(columns = ['next_pymnt_d','mths_since_last_record'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Cleanup text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lend_df.describe(include ='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term column should have numeric data , find all text and replace and change dtype\n",
    "print(len(pd_lend_df[pd_lend_df['term'].str.contains(\"months\") == True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lend_df['term'] = pd_lend_df['term'].str.replace(\"months\" ,\"\")\n",
    "pd_lend_df['term'] = pd_lend_df['term'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_rate seems to have % symbol, find out , replace and change type\n",
    "print(len(pd_lend_df[pd_lend_df['int_rate'].str.contains(\"%\") == True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lend_df['int_rate'] = pd_lend_df['int_rate'].str.replace(\"%\" ,\"\")\n",
    "pd_lend_df['int_rate'] = pd_lend_df['int_rate'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # Change type to date and date-time as applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lend_df['issue_d'] = pd.to_datetime(pd_lend_df['issue_d'],format = '%b-%y' )\n",
    "pd_lend_df['last_pymnt_d'] = pd.to_datetime(pd_lend_df['last_pymnt_d'],format = '%b-%y' )\n",
    "pd_lend_df['last_credit_pull_d'] = pd.to_datetime(pd_lend_df['last_credit_pull_d'],format = '%b-%y' )\n",
    "pd_lend_df['earliest_cr_line'] = pd.to_datetime(pd_lend_df['earliest_cr_line'],format = '%b-%y' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relook at object columns\n",
    "pd_lend_df.describe(include ='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revol_util also have % symbol, drop that and update type , seems this is only such column now\n",
    "print(len(pd_lend_df[pd_lend_df['revol_util'].str.contains(\"%\") == True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lend_df['revol_util'] = pd_lend_df['revol_util'].str.replace(\"%\" ,\"\")\n",
    "pd_lend_df['revol_util'] = pd_lend_df['revol_util'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emp_length should have values between 0 to 10 , cleanup text and change type\n",
    "print(len(pd_lend_df[pd_lend_df['emp_length'].str.contains(\"years\") == True]))\n",
    "print(pd_lend_df['emp_length'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lend_df['emp_length'] = pd_lend_df['emp_length'].str.replace(\"< 1\" ,\"0\")\n",
    "pd_lend_df['emp_length'] = pd_lend_df['emp_length'].str.replace(\"+\" ,\"\")\n",
    "pd_lend_df['emp_length'] = pd_lend_df['emp_length'].str.replace(\"years\" ,\"\")\n",
    "pd_lend_df['emp_length'] = pd_lend_df['emp_length'].str.replace(\"year\" ,\"\")\n",
    "pd_lend_df['emp_length'] = pd_lend_df['emp_length'].str.replace(\" \" ,\"\")\n",
    "print(pd_lend_df['emp_length'].unique())\n",
    "#pd_lend_df.describe(include ='object')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan emp_length mostly means 0 , lets analyze  \n",
    "fig = px.pie(pd_lend_df, names='emp_length')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 is second largest value , lets replace nan with 0  and change column type\n",
    "pd_lend_df['emp_length'] = pd_lend_df['emp_length'].fillna(0)\n",
    "pd_lend_df['emp_length']  = pd_lend_df['emp_length'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # # URL column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url column is giving clutterred view when described, lets analyze\n",
    "import urllib.parse as urlparse\n",
    "pd_lend_df['protocol'],pd_lend_df['domain'],pd_lend_df['path'],pd_lend_df['query'],pd_lend_df['fragment'] = zip(*pd_lend_df['url'].map(urlparse.urlsplit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lend_df[['protocol','domain','path','query' ,'fragment']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop url and components with unique values and replace loan-id text and make it integer\n",
    "pd_lend_df.drop(columns = ['protocol','domain','path','fragment','url'] ,inplace = True)\n",
    "pd_lend_df['query'] = pd_lend_df['query'].str.replace(\"loan_id=\",\"\")\n",
    "pd_lend_df['query']  = pd_lend_df['query'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if id and loan id are same\n",
    "pd_lend_df['diffce'] = pd_lend_df['query'] - pd_lend_df['id']\n",
    "pd_lend_df['diffce'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop calculation column created above\n",
    "pd_lend_df.drop(columns=['diffce'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop query column\n",
    "pd_lend_df.drop(columns=['query'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lend_df.describe(include ='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems some object columns can be converted to category ,\n",
    "pd_lend_df['grade'] = pd_lend_df['grade'].astype('category')\n",
    "pd_lend_df['home_ownership'] = pd_lend_df['home_ownership'].astype('category')\n",
    "pd_lend_df['purpose'] = pd_lend_df['purpose'].astype('category')\n",
    "pd_lend_df['addr_state'] = pd_lend_df['addr_state'].astype('category')\n",
    "pd_lend_df['verification_status'] = pd_lend_df['verification_status'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also title and purpose seem to have similar values , title having lot unique values and  purpose with few categories, drop it \n",
    "# from data dictionary , we know that emp_title is mix of employer name and title , it has lot of quniue values, lets drop\n",
    "pd_lend_df.drop(columns=['emp_title','title'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lend_df.describe(include ='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # # Cleanup desc column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lend_df['desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems too long text in desc column\n",
    "pd_lend_df['desc'].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It seems long text with purpose plus some info. , might not be relevant for our analysis , lets drop to save space\n",
    "pd_lend_df.drop(columns=['desc'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # # zip_code clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip code should have 3 digit nos, remove xxx and find relationship addr state\n",
    "pd_lend_df['zip_code'] = pd_lend_df['zip_code'].str.replace(\"xx\" ,\"\")\n",
    "pd_lend_df.groupby('addr_state')['zip_code'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# none of the zip codes have enough records ( > 30%) , dropping it\n",
    "pd_lend_df.drop(columns=['zip_code'], inplace = True)\n",
    "pd_lend_df.describe(include ='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub grade and grade are related and sub_grade is grade + number , lets remove text\n",
    "pd_lend_df['sub_grade'] = pd_lend_df['sub_grade'].str[1:]\n",
    "pd_lend_df['sub_grade'] = pd_lend_df['sub_grade'].astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # Drop more unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lend_df.drop(columns=['funded_amnt_inv','total_pymnt_inv','total_pymnt','pub_rec_bankruptcies'], inplace = True)\n",
    "                       # 'mths_since_last_delinq','total_rec_late_fee','recoveries','total_rec_late_fee','collection_recovery_fee']\n",
    "\n",
    "# for now keep -- these columns are post charged off -- 'mths_since_last_delinq','total_rec_late_fee','recoveries','total_rec_late_fee','collection_recovery_fee'\n",
    "# _inv is for investors contribution, it is highly correlated with other columns with similar numeric data not relevant for this exercise at this moment\n",
    "# total_pymnt = total_rec_prncp + total_rec_int , hence dropping                      \n",
    "# pub_rec_bankruptcies kind of same as pub_rec and comes much later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# relook at info\n",
    "pd_lend_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Look and cleanup of numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze numeric columns\n",
    "pd_lend_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd_lend_df['id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd_lend_df['member_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both id and member_id are unique , member_id is actual applicant id , so lets keep it as index  and drop id\n",
    "pd_lend_df.drop(columns = ['id'] , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lend_df.set_index('member_id' , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delinq_2yrs , inq_last_6mths , pub_rec  , mths_since_last_delinq  might be categorical and/or int columns\n",
    "pd_lend_df['delinq_2yrs'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lend_df['inq_last_6mths'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lend_df['pub_rec'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # # # Treat Na values in mths_since_last_delinq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mths_since_last_delinq nan means either no records or no delinquency\n",
    "pd_lend_df['mths_since_last_delinq'] = pd_lend_df['mths_since_last_delinq'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lend_df['delinq_2yrs'] = pd_lend_df['delinq_2yrs'].astype(int).astype('category')\n",
    "pd_lend_df['inq_last_6mths'] = pd_lend_df['inq_last_6mths'].astype(int).astype('category')\n",
    "pd_lend_df['pub_rec'] = pd_lend_df['pub_rec'].astype(int).astype('category')\n",
    "pd_lend_df['mths_since_last_delinq'] = pd_lend_df['mths_since_last_delinq'].astype('int')\n",
    "#fnor lets make it categorical , later see how to handle nan values\n",
    "pd_lend_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # # # Not sure of floating point precision, round it to 2 for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = pd_lend_df.select_dtypes(include=[float]).columns\n",
    "df_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each_float_column in df_num:\n",
    "    pd_lend_df[each_float_column] = round(pd_lend_df[each_float_column],2)\n",
    "pd_lend_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find raw correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = pd_lend_df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Except loan_amnt, funded_amnt , installment , total_rec_prncp , total_rec_int which are highly related , rest of the columns do not seem to  strong positive and/or negative correlation.\n",
    "Most of the negative correlations are very weak(no correlation) and positive correlations are either very weak(no correlation)  or weak to medium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For many numeric variables, box plots is needed, lets modularize it using a function\n",
    "def plot_box_plot_univariate(univariate_col_y):\n",
    "    fig = px.box(pd_lend_df, y=univariate_col_y , title = \"box plot for \" + univariate_col_y)\n",
    "    fig.update_traces(quartilemethod=\"linear\") # or \"inclusive\", or \"linear\" by default\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_num = pd_lend_df.select_dtypes(include=[np.number]).columns\n",
    "for each_numeric_column in df_num:\n",
    "    plot_box_plot_univariate(each_numeric_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # Outliar treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # it seems there are some columns that have outliars,\n",
    "#for now , among input columns annual_inc is imp. , lets treat outliars\n",
    "# rest we will see later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#annual income seems to have some outliars  , lets analyze before treating them above  ~1.5* q3\n",
    "pd_annual_inc_out = pd_lend_df[pd_lend_df['annual_inc'] >= 250000][['loan_amnt','loan_status','annual_inc','emp_length']].copy()\n",
    "#pd_annual_inc_out = pd_annual_inc_out[pd_annual_inc_out['loan_status'] == 'Charged Off']\n",
    "#print(len(pd_annual_inc_out) , print(len(pd_annual_inc_out[pd_annual_inc_out['loan_status'] == 'Charged Off'])))\n",
    "pd_annual_inc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pd_annual_inc_out[pd_annual_inc_out['loan_status'] == 'Charged Off']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for >0.25M income group , total no. of loans and the loan amonuts are not high and only few loans Charged off , hence replacing outliars with median\n",
    "#Also adding a column to identify if outliars are adjusted\n",
    "pd_lend_df['annual_inc_outliar_adjust'] = np.where(pd_lend_df['annual_inc'] >= 250000, pd_lend_df['annual_inc'],0)\n",
    "pd_lend_df['annual_inc'] = np.where(pd_lend_df['annual_inc'] >= 250000, pd_lend_df['annual_inc'].median(),pd_lend_df['annual_inc'])\n",
    "fig = px.box(pd_lend_df, y=\"annual_inc\")\n",
    "fig.update_traces(quartilemethod=\"linear\") # or \"inclusive\", or \"linear\" by default\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Univariate analysis categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_pie_chart_categorical_columns(category_col_name):\n",
    "    #fig = px.pie(pd_lend_df, names=category_col_name , title = \"box plot  \" + category_col_name) # Add title to plots\n",
    "    #fig.show()\n",
    "    #For now only plotting histograms but box plots can be plotted easily\n",
    "    fig = px.histogram(pd_lend_df, x= category_col_name, title = \"histogram \" + category_col_name)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cat_cols = pd_lend_df.select_dtypes(include=['category']).columns\n",
    "print(cat_cols)\n",
    "for cat_col in cat_cols:\n",
    "    print(\"\")\n",
    "    plot_histogram_pie_chart_categorical_columns(cat_col)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # Rank Frequency plots ordered categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_freq_plots_ordered_categorical(column_name):\n",
    "    df_rank_freq = pd_lend_df[column_name].value_counts(ascending = False).to_frame().reset_index()\n",
    "    df_rank_freq.rename(columns = {'index' : 'label'},inplace=True)\n",
    "    df_rank_freq['rank'] = df_rank_freq.index + 1\n",
    "    fig = px.line(df_rank_freq, x='rank', y=column_name , title='Rank-Freq plot for ' + column_name, log_x=True, log_y=True)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_freq_plots_ordered_categorical('purpose')\n",
    "rank_freq_plots_ordered_categorical('addr_state')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmented univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine impact of a random variable on loan_status to determine thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charged_off_pct = round(100*(len(pd_lend_df[pd_lend_df['loan_status'] == \"Charged Off\"])/len(pd_lend_df)),2)\n",
    "charged_off_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charged_off_pct_upper = 15\n",
    "charged_off_pct_lower = 14 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_segmented_univariate(column_name):\n",
    "    table1 = pd.pivot_table(pd_lend_df[['loan_status',column_name]], index = column_name ,columns=['loan_status']\n",
    "                            , aggfunc=np.size)\n",
    "    table1.columns = table1.columns.astype(str)\n",
    "    table1['%ChargedOff'] = round(100*(table1['Charged Off']/(table1['Charged Off'] + table1['Fully Paid'])) ,2)\n",
    "    table1['ChargedOff_pct_diff_upper'] = table1['%ChargedOff'] - charged_off_pct_upper\n",
    "    table1['ChargedOff_pct_diff_lower'] = table1['%ChargedOff'] - charged_off_pct_lower\n",
    "    table1.sort_values(by='%ChargedOff', ascending = False)\n",
    "    return(table1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_lend_df['random'] = np.random.random((len(pd_lend_df),1))\n",
    "pd_lend_df['random'] = np.where(pd_lend_df['random']>=0.5,\"H\",\"T\")\n",
    "table1 = pivot_segmented_univariate('random')\n",
    "table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # After few trials The impact of random event is around 0.6% , \n",
    "# #  hence we keep the upper and lower thresholds  to upper and lower integers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_segmented_univariate_numeric(num_column_name):\n",
    "    fig = px.box(pd_lend_df, x= num_column_name,y=\"loan_status\" )\n",
    "    # target variable is loan_status, for this exercise it is constant, so hardcoded\n",
    "    fig.update_traces(quartilemethod=\"linear\") # or \"inclusive\", or \"linear\" by default\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = pd_lend_df.select_dtypes(include=[np.number]).columns\n",
    "for each_numeric_column in df_num:\n",
    "    plot_segmented_univariate_numeric(each_numeric_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segmented univariate analysis on categorical columns\n",
    "cat_cols = pd_lend_df.select_dtypes(include=['category']).columns\n",
    "for cat_col in cat_cols:\n",
    "    if cat_col == 'loan_status':\n",
    "        continue\n",
    "    table1 = pivot_segmented_univariate(cat_col)\n",
    "    print(\"\")\n",
    "    print(\"pivot table for loan_status vs  \" , cat_col )\n",
    "    display(table1)\n",
    "    print( \"-------------------------------------------------------------------------------------------\")\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert numeric columns to categorical and perform segmented analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post charged off columns are excluded from this\n",
    "def convert_numeric_column_to_categorical_analyze(numeric_column):\n",
    "    q=[0, .05, .25, .5, .75, 0.95,1]\n",
    "    labels = [5,25,50,75,95,100]\n",
    "    pd_lend_df['binned_' + numeric_column] = pd.qcut(pd_lend_df[numeric_column], q=q, labels=labels)\n",
    "    table1 = pivot_segmented_univariate('binned_' + numeric_column)\n",
    "    print(\"\")\n",
    "    print(\"pivot table for loan_status vs  \" , 'binned_' + numeric_column )\n",
    "    display(table1)\n",
    "    print( \"-------------------------------------------------------------------------------------------\")\n",
    "    print(\"\")\n",
    "    pd_lend_df.drop(columns = ['binned_' + numeric_column],inplace = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = pd_lend_df.select_dtypes(include=[np.number]).columns\n",
    "columns_to_exclude = ['pub_rec','inq_last_6mths','mths_since_last_delinq','total_rec_late_fee','recoveries','total_rec_late_fee','collection_recovery_fee','annual_inc_outliar_adjust','delinq_2yrs']\n",
    "for each_numeric_column in df_num:\n",
    "    if each_numeric_column in columns_to_exclude:\n",
    "        continue\n",
    "    print(each_numeric_column)\n",
    "    convert_numeric_column_to_categorical_analyze(each_numeric_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmented Univariate analysis : Mean and Std comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe this analysis is redundant since we are anyways plotting medians and quartiles\n",
    "# neverthesess as practice completing this for alll columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_exclude = ['annual_inc_outliar_adjust']\n",
    "df_num = pd_lend_df.select_dtypes(include=[np.number]).columns\n",
    "for each_numeric_column in df_num:\n",
    "    if each_numeric_column in columns_to_exclude:\n",
    "        continue\n",
    "    mean_numeric_column = pd_lend_df[each_numeric_column].mean()\n",
    "    std_numeric_column = pd_lend_df[each_numeric_column].std()\n",
    "    print(\"------------------------------------------------------------------------------------\")\n",
    "    print(each_numeric_column , pd_lend_df[each_numeric_column].describe())\n",
    "    print(\"After segmentation by loan status\")\n",
    "    print(pd_lend_df.groupby('loan_status')[each_numeric_column].describe()[['count','mean','std','25%','50%','75%']])\n",
    "    print(\"\")\n",
    "    print(\"------------------------------------------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bivariate_pivot_table(column1, column2):\n",
    "    table1 = pd.pivot_table(pd_lend_df[['loan_status',column1 , column2]], index = [column1 , column2],columns=['loan_status']\n",
    "                            , aggfunc=np.size)\n",
    "    table1.columns = table1.columns.astype(str)\n",
    "    table1['%ChargedOff'] = round(100*(table1['Charged Off']/(table1['Charged Off'] + table1['Fully Paid'])) ,2)\n",
    "    table1 = table1[table1['%ChargedOff'].notna()]\n",
    "    table1['ChargedOff_pct_diff_upper'] = table1['%ChargedOff'] - charged_off_pct_upper\n",
    "    table1['ChargedOff_pct_diff_lower'] = table1['%ChargedOff'] - charged_off_pct_lower\n",
    "    table1.sort_values(by='%ChargedOff', ascending = False)\n",
    "    return(table1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = pd_lend_df.select_dtypes(include=['category']).columns\n",
    "for cat_col1 in cat_cols:\n",
    "    for cat_col2 in cat_cols:\n",
    "        if cat_col1 == 'loan_status' or cat_col1 == cat_col2 or cat_col2 == 'loan_status':\n",
    "            continue\n",
    "        table1 = bivariate_pivot_table(cat_col1,cat_col2)\n",
    "        print(\"\")\n",
    "        print(\"pivot table for loan_status vs  \" , cat_col1 , cat_col2 )\n",
    "        display(table1)\n",
    "        print( \"-------------------------------------------------------------------------------------------\")\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numeric_column_to_categorical_analyze_bivariate(numeric_column1,numeric_column2):\n",
    "    q=[0, .05, .25, .5, .75, 0.95,1]\n",
    "    labels = [5,25,50,75,95,100]\n",
    "    pd_lend_df['binned_' + numeric_column1] = pd.qcut(pd_lend_df[numeric_column1], q=q, labels=labels)\n",
    "    pd_lend_df['binned_' + numeric_column2] = pd.qcut(pd_lend_df[numeric_column1], q=q, labels=labels)\n",
    "    table1 = bivariate_pivot_table('binned_' + numeric_column1,'binned_' + numeric_column2)\n",
    "    print(\"\")\n",
    "    print(\"pivot table for loan_status vs  \" , 'binned_' + numeric_column1 , 'binned_' + numeric_column2 )\n",
    "    display(table1)\n",
    "    print( \"-------------------------------------------------------------------------------------------\")\n",
    "    print(\"\")\n",
    "    pd_lend_df.drop(columns = ['binned_' + numeric_column1,'binned_' + numeric_column2],inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numeric_column_to_categorical_analyze_bivariate_existing_categorical(numeric_column):\n",
    "    q=[0, .05, .25, .5, .75, 0.95,1]\n",
    "    labels = [5,25,50,75,95,100]\n",
    "    pd_lend_df['binned_' + numeric_column] = pd.qcut(pd_lend_df[numeric_column], q=q, labels=labels)\n",
    "    for cat_col in cat_cols:\n",
    "        print(cat_col)\n",
    "        if cat_col == 'loan_status':\n",
    "            continue\n",
    "        table1 = bivariate_pivot_table('binned_' + numeric_column,cat_col)\n",
    "        print(\"\")\n",
    "        print(\"pivot table for loan_status vs  \" , 'binned_' + numeric_column , cat_col , \" category\")\n",
    "        display(table1)\n",
    "        print( \"-------------------------------------------------------------------------------------------\")\n",
    "        print(\"\")\n",
    "    pd_lend_df.drop(columns = ['binned_' + numeric_column],inplace = True)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = pd_lend_df.select_dtypes(include=[np.number]).columns\n",
    "columns_to_exclude = ['pub_rec','inq_last_6mths','mths_since_last_delinq','total_rec_late_fee','recoveries','total_rec_late_fee','collection_recovery_fee','annual_inc_outliar_adjust','delinq_2yrs']\n",
    "for each_numeric_column1 in df_num:\n",
    "    for each_numeric_column2 in df_num:\n",
    "        if each_numeric_column1 in columns_to_exclude or each_numeric_column1 == each_numeric_column2 or each_numeric_column2 in columns_to_exclude:\n",
    "            continue\n",
    "        convert_numeric_column_to_categorical_analyze_bivariate(each_numeric_column1,each_numeric_column2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = pd_lend_df.select_dtypes(include=[np.number]).columns\n",
    "print(df_num)\n",
    "columns_to_exclude = ['pub_rec','inq_last_6mths','mths_since_last_delinq','total_rec_late_fee','recoveries','total_rec_late_fee','collection_recovery_fee','annual_inc_outliar_adjust','delinq_2yrs']\n",
    "for each_numeric_column in df_num:\n",
    "    if each_numeric_column in columns_to_exclude:\n",
    "        continue\n",
    "    print(\" processing \" , each_numeric_column)\n",
    "    convert_numeric_column_to_categorical_analyze_bivariate_existing_categorical(each_numeric_column)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derived Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert object to dates\n",
    "pd_lend_df['issue_d'] = pd.to_datetime(pd_lend_df['issue_d'],format = '%b-%y' )\n",
    "pd_lend_df['last_pymnt_d'] = pd.to_datetime(pd_lend_df['last_pymnt_d'],format = '%b-%y' )\n",
    "pd_lend_df['last_credit_pull_d'] = pd.to_datetime(pd_lend_df['last_credit_pull_d'],format = '%b-%y' )\n",
    "pd_lend_df['earliest_cr_line'] = pd.to_datetime(pd_lend_df['earliest_cr_line'],format = '%b-%y' )\n",
    "#print(type(pd_lend_df['issue_d']), type(pd_lend_df['last_pymnt_d']))\n",
    "pd_lend_df['issue_d'].head()\n",
    "pd_lend_df['last_pymnt_d'].head()\n",
    "pd_lend_df['last_credit_pull_d'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd_annual_inc_out = pd_lend_df[pd_lend_df['revol_bal'] >= 60000][['loan_amnt','loan_status','revol_bal','revol_util']].copy()\n",
    "#pd_annual_inc_out = pd_annual_inc_out[pd_annual_inc_out['loan_status'] == 'Charged Off']\n",
    "print(pd_annual_inc_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd_annual_inc_out = pd_lend_df[pd_lend_df['revol_util'] >= 75][['loan_amnt','loan_status','revol_bal','revol_util']].copy()\n",
    "pd_annual_inc_out = pd_annual_inc_out[pd_annual_inc_out['loan_status'] == 'Charged Off']\n",
    "print(pd_annual_inc_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd_annual_inc_out = pd_lend_df[((pd_lend_df['revol_util'] >= 75) & (pd_lend_df['revol_bal'] >= 60000))][['loan_amnt','loan_status','revol_bal','revol_util']].copy()\n",
    "#pd_annual_inc_out = pd_annual_inc_out[pd_annual_inc_out['loan_status'] == 'Charged Off']\n",
    "print(len(pd_annual_inc_out),len(pd_annual_inc_out[pd_annual_inc_out['loan_status'] == 'Charged Off']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand how sub grade 1,5 are structured\n",
    "pd_lend_df['sub_grade'] = pd_lend_df['sub_grade'].str[1:]\n",
    "pd_lend_df['sub_grade'] = pd_lend_df['sub_grade'].str[1:]\n",
    "table1 = pd.pivot_table(pd_lend_df[['loan_status','sub_grade']], index = 'sub_grade',columns=['loan_status'] , aggfunc=np.size)\n",
    "table1['%ChargedOff'] = round(100*(table1['Charged Off']/(table1['Charged Off'] + table1['Fully Paid'])) ,2)\n",
    "table1.sort_values(by='%ChargedOff', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd_lend_df['Ratio_funded_to_applied'] = pd_lend_df['funded_amnt']/pd_lend_df['loan_amnt']\n",
    "new_df = pd_lend_df[pd_lend_df['Ratio_funded_to_applied'] <1]\n",
    "print(len(new_df))\n",
    "fig = px.box(new_df, x=\"Ratio_funded_to_applied\")\n",
    "fig.update_traces(quartilemethod=\"linear\") # or \"inclusive\", or \"linear\" by default\n",
    "fig.show()\n",
    "pd_lend_df['Ratio_funded_to_applied'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" run complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
